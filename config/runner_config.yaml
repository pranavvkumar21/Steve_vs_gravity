runner:
  class_name: OnPolicyRunner
  # General
  num_steps_per_env: 64  # Number of steps per environment per iteration
  max_iterations: 8000 # Number of policy updates
  seed: 42
  # Observations
  obs_groups: {"policy": ["policy"], "critic": ["policy", "privileged"]} # Maps observation groups to sets. See `vec_env.py` for more information
  # Logging parameters
  save_interval: 50  # Check for potential saves every `save_interval` iterations
  experiment_name: make_steve_walk
  run_name: "steve_godspeed"
  # Logging writer
  logger: tensorboard  # tensorboard, neptune, wandb


  # Policy
  policy:
    class_name: ActorCritic
    activation: elu
    actor_obs_normalization: true
    critic_obs_normalization: true
    actor_hidden_dims: [1024, 512, 256]
    critic_hidden_dims: [1024, 512, 256]
    init_noise_std: -2.0
    noise_std_type: "log"  # 'scalar' or 'log'
    state_dependent_std: false

  # Algorithm
  algorithm:
    class_name: PPO
    # Training
    learning_rate: 3.5e-4
    num_learning_epochs: 4
    num_mini_batches: 4  # mini batch size = num_envs * num_steps / num_mini_batches
    schedule: adaptive  # adaptive, fixed
    # Value function
    value_loss_coef: 2.8
    clip_param: 0.2
    use_clipped_value_loss: true
    # Surrogate loss
    desired_kl: 0.015
    entropy_coef: 0.01
    gamma: 0.992
    lam: 0.95
    max_grad_norm: 1.0
    # Miscellaneous
    normalize_advantage_per_mini_batch: true

